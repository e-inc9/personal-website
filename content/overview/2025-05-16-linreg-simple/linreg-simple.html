---
title: "Simple linear regression"
output: html_document
date: '2025-05-16'
categories:
  - General
editor_options: 
  markdown: 
    wrap: 72
---



<div id="mathematical-and-statistical-relations" class="section level2">
<h2>Mathematical and statistical relations</h2>
<p>Consider two variables <span class="math inline">\(x,y\)</span> that you know to be related.</p>
<ul>
<li><p>They are related <em>mathematically</em> if there exists a <em>deterministic</em>
relationship between the two, such that <span class="math inline">\(x\)</span> perfectly determines <span class="math inline">\(y\)</span>
and vice versa. Such relationships can be expressed with a function
<span class="math inline">\(Y=f(X)\)</span>. One example is the relationship between inches and
centimeters <span class="math inline">\(Y=2.54X\)</span>. A man who is <span class="math inline">\(x=65\)</span> inches tall must be
<span class="math inline">\(y=165.1\)</span> centimeters tall.</p></li>
<li><p>They are related <em>statistically</em> if there exists a relationship
between the two, but <span class="math inline">\(X\)</span> does <em>not</em> perfectly determine <span class="math inline">\(Y\)</span> and vice
versa. You know the two are related in some way – whether visually
or intuitively – but you cannot say you know everything about <span class="math inline">\(Y\)</span>
just by <span class="math inline">\(X\)</span>. Take the below scatter plot relating <span class="math inline">\(X\)</span>, the velocity
at which a bullet is fired, and <span class="math inline">\(Y\)</span>, the depth of penetration into
body armor. Perhaps you didn’t even have to look at the below graph
to know that a relationship exists. Faster bullet <span class="math inline">\(\implies\)</span> deeper
penetration. Yet the presence of other factors at play – angle,
wind speed, freak properties of physics – make you unable to
perfectly determine penetration given velocity.</p></li>
</ul>
<p>Does that mean we should just <em>give up</em>? Forget fitting a mathematical
equation to this relationship? Get scared in the face of unknowing,
uncertainty, the potential of being <em>wrong</em>? Hell no. Patterns exist,
trends lurk beneath; you can fit some rules, induce some structure, make
some sense of it all – potential life lesson there…</p>
<p><img src="/overview/2025-05-16-linreg-simple/linreg-simple_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
</div>
<div id="building-up-to-the-simple-linear-regression-model" class="section level1">
<h1>Building up to the simple linear regression model</h1>
<p>Our model is best understood as a <em>formal counterpart</em> for two phenomena
observed in statistical relations: 1) the tendency of <span class="math inline">\(Y\)</span> to vary with <span class="math inline">\(X\)</span> yet 2) the inexactitude of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>We can translate these observations to a probabilistic framework by saying that at a given level of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> will follow a probabilistic distribution. Furthermore, the mean of this distribution will differ with (moves with) <span class="math inline">\(X\)</span>.</p>
<p><img src="../../../../../../../../pics/3d_distrib_linreg.png" /><!-- --></p>
<p>In the case of simple linear regression, we’ll make some assumptions about the nature of this distribution. For now, note that the mean of our distributions always lie along a straight line.</p>
<div id="basic-setup" class="section level2">
<h2>Basic setup</h2>
<p>Our regression model starts off similar to that of a mathematical
function <span class="math inline">\(Y=f(X)\)</span>. However, we add an <em>error variable</em>, <span class="math inline">\(\varepsilon_i\)</span>,
to account for both randomness and the existence of <em>other variables</em> that make the
relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> imperfect.</p>
<p>This model assumes a relationship exists between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, but
concedes that it is non-mathematical and that <span class="math inline">\(Y\)</span> is influenced by
factors other than <span class="math inline">\(X\)</span>. It is the formal counterpart to what we know
from the graph and our intuition.</p>
<p><span class="math display">\[ Y =f(X) + \varepsilon \]</span></p>
</div>
<div id="adding-assumption-of-linearity" class="section level2">
<h2>Adding assumption of linearity</h2>
<p>At this stage, we must define <span class="math inline">\(f(X)\)</span>, the nature of the relationship
between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It can’t just be a black box function. Returning to
the graph above, we could say that penetration increases linearly with
velocity. The scatter does appear to mostly be a straight blob and not a
curvy one. It’s simple and straightforward. <span class="math inline">\(\beta_1\)</span> is the degree to
which penetration increases when velocity increases by 1 m/sec.
<span class="math inline">\(\beta_0\)</span> is penetration when velocity is 0. For most purposes,
<span class="math inline">\(\beta_0\)</span> isn’t useful, but since we are modelling a linear
relationship, there must be a <span class="math inline">\(y\)</span>-intercept. Note that we do not
dispense with <span class="math inline">\(\varepsilon_i\)</span>; the assumption is still that there exist
randomness and other variables affecting <span class="math inline">\(Y\)</span>, but <span class="math inline">\(X\)</span>’s effect on <span class="math inline">\(Y\)</span> is linear.
<span class="math display">\[ Y_i = \beta_0 + \beta_1X_i + \varepsilon_i \]</span></p>
<p>We assume that <span class="math inline">\(X_i\)</span> are known constants.</p>
</div>
<div id="on-the-error-variable" class="section level2">
<h2>On the error variable</h2>
<p><span class="math inline">\(\varepsilon_i\)</span> incorporates randomness and other variables. By definition, it is a random variable. Since <span class="math inline">\(X_i\)</span> are known constants, and <span class="math inline">\(\beta_0, \beta_1\)</span> are constants also, this <span class="math inline">\(\varepsilon_i\)</span> is what makes <span class="math inline">\(Y_i\)</span> a random variable and how we account for the inexactitude of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>The following assumptions for <span class="math inline">\(\varepsilon_i\)</span> are crucial:</p>
<ul>
<li><span class="math inline">\(\varepsilon_i\)</span> follows a distribution centered at 0 with
variance <span class="math inline">\(\sigma^2\)</span>.</li>
<li>Thus, <span class="math inline">\(E(\varepsilon)\)</span> = 0.</li>
<li><span class="math inline">\(\varepsilon_i\)</span> is uncorrelated; one <span class="math inline">\(\varepsilon_i\)</span> is not related to
another.</li>
</ul>
</div>
<div id="the-resulting-model" class="section level2">
<h2>The resulting model</h2>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(E(Y_i) = E(\beta_0+\beta_1X_i+\varepsilon) = \beta_0 + \beta_1X_i + 0\)</span>.
The expected value of <span class="math inline">\(Y_i\)</span> is solely dependent on <span class="math inline">\(X_i\)</span>. Our fitted <span class="math inline">\(\beta_0, \beta_1\)</span>, are used to predict the expected value of <span class="math inline">\(Y_i\)</span> at <span class="math inline">\(X_i\)</span>.</p></li>
<li><p><span class="math inline">\(V(Y_i) = V(\beta_0+\beta_1X_i+\varepsilon_i) = V(\varepsilon_i) = \sigma^2\)</span>. We expect <span class="math inline">\(Y_i\)</span> to vary about <span class="math inline">\(E(Y_i)\)</span> by the variance of <span class="math inline">\(\varepsilon_i\)</span> <em>at all levels of <span class="math inline">\(X_i\)</span></em>.</p></li>
<li><p>Since <span class="math inline">\(\varepsilon_i\)</span> is uncorrelated with <span class="math inline">\(\varepsilon_j\)</span>, <span class="math inline">\(Y_i, Y_j\)</span> are uncorrelated as well.</p></li>
</ol>
<p>Summarily, <span class="math inline">\(Y_i\)</span> follows a distribution centered at <span class="math inline">\(\beta_0 + \beta_1X_i\)</span> with variance of <span class="math inline">\(\sigma^2\)</span>. This tendency to vary comes from error term <span class="math inline">\(\varepsilon_i\)</span>. <span class="math inline">\(\beta_1\)</span> gives the change in the mean of this distribution with <span class="math inline">\(X_i\)</span>; it tells us how we’d <em>expect</em> <span class="math inline">\(Y_i\)</span> to change with <span class="math inline">\(X_i\)</span>. Any departures from <span class="math inline">\(E(Y_i) = \beta_0 + \beta_1X_i\)</span> are due to <span class="math inline">\(\varepsilon_i\)</span>’s tendency to vary.</p>
</div>
<div id="estimating-regression-coefficients" class="section level2">
<h2>Estimating regression coefficients</h2>
<p>If there exist sample data for which we think the above model applies, we then use
that data to best estimate coefficients <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span>. Recall that these two parameters are used to compute the expected <span class="math inline">\(Y_i\)</span> at <span class="math inline">\(X_i\)</span>. Thus, <span class="math inline">\(\hat Y_i\)</span> is also an estimate of the expected <span class="math inline">\(Y_i\)</span> at <span class="math inline">\(X_i\)</span>.</p>
<p>A general rule of thumb for fitting a linear regression line is that it should
run through our sample points such that the absolute distance between
the line and the points in our data is minimized. This is called the
<em>ordinary least squares (OLS)</em> method. We can express this distance as a
sum of squared differences:</p>
<p><span class="math display">\[  \sum_{i=1}^{n} (Y_i - \hat Y_i)^2 = \sum_{i=1}^n(Y_i - (\hat \beta_0 + \hat \beta_1 X_i))^2 \]</span>
Considering that this is a convex function, taking the first derivative
of it with respect to <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span>, equating both
results to 0, and solving the system of equations give us the
<span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span> where the sum of squared differences is
minimized:</p>
<p><span class="math display">\[ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X\]</span>
<span class="math display">\[ \hat \beta_1 = \frac{\sum (X_iY_i) - n \bar X \bar Y}{ \sum X_i^2 - n \bar X^2} \]</span></p>
<p><a href="/proofs_deriv/2025/05/19/deriving-the-coefficients-for-linear-regression/">Full derivation here, for those
interested.</a></p>
<p>Under the <em>Gauss-Markov theorem</em>, the OLS method is the <em>best</em> way of estimating <span class="math inline">\(\beta_0, \beta_1\)</span> because our estimates will be <a href="/overview/2025-09-13-point-estimation/point-estimation/">a) unbiased and b) have minimum variance out of all unbiased <strong>linear</strong> estimators.</a>. We also arrive at the same expressions for <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span> when we use <a href="/overview/2025-09-15-maximum-likelihood-estimation/mle/">maximum likelihood estimation</a>.</p>
<p>Given <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span>, we have</p>
<p><span class="math display">\[ \hat{Y} = \hat \beta_0 + \hat \beta_1X \]</span></p>
<p>which gives us the expected or mean point estimate <span class="math inline">\(\hat Y_i\)</span> for a given <span class="math inline">\(X_i\)</span>. This point estimate is our fitted value. It is liable to differ from our observed value <span class="math inline">\(Y_i\)</span> by <span class="math inline">\(e_i = Y_i - \hat Y_i\)</span>, or the <span class="math inline">\(i^{th}\)</span> residual.</p>
</div>
<div id="residuals" class="section level2">
<h2>Residuals</h2>
<p>Note that <span class="math inline">\(e_i\)</span>, the residual corresponding to a given <span class="math inline">\(Y_i\)</span>, is different from <span class="math inline">\(\varepsilon_i\)</span>, the random variable.</p>
<p>We cannot know <span class="math inline">\(\varepsilon_i\)</span> since it measures deviation from a “true” regression line which itself is unknown. We just know it’s a random variable.</p>
<p>On the other hand, we know <span class="math inline">\(e_i\)</span> as a measure of deviation from a <em>fitted</em> regression line. It’s downstream of our sample.</p>
<p>Some properties of <span class="math inline">\(e_i\)</span> to note.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sum e_i^2\)</span> is at its minimum. This is what the least squares method does.</li>
<li><span class="math inline">\(\sum e_i = 0\)</span>. The sum of residuals is 0.</li>
<li><span class="math inline">\(\sum Y_i\)</span> = <span class="math inline">\(\sum \hat Y_i\)</span>.</li>
<li><span class="math inline">\(\sum \hat Y_i e_i\)</span> = <span class="math inline">\(\sum X_i e_i = 0\)</span>. The sum of residuals weighted by fitted values or levels of X is 0.</li>
<li>The fitted regression line goes through <span class="math inline">\((\bar X, \bar Y)\)</span>.</li>
</ol>
<p>All of these are downstream from the <span class="math inline">\(\hat \beta_0, \hat \beta_1\)</span> we found above.</p>
</div>
<div id="returning-to-the-error-variable" class="section level2">
<h2>Returning to the error variable</h2>
<p>Earlier, we stated that at each level of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> follows a distribution centered at <span class="math inline">\(E(Y_i)\)</span> with variance <span class="math inline">\(\sigma^2\)</span>. Given that this variance corresponds to the ways in which observed values may deviate about a “true” regression line, it’s important to get an idea of <em>its size</em>.</p>
<p>Given a sample, we estimate the variance of <span class="math inline">\(Y_i\)</span> the same way we would estimate the variance of a single population but with one difference: we divide by <span class="math inline">\(n-2\)</span>, since two parameters <span class="math inline">\(\beta_0, \beta_1\)</span> are estimated.</p>
<p>Therefore, our variance estimator is:</p>
<p><span class="math display">\[
MSE = \text{Error/Residual mean square}= s^2 = \frac{\sum(Y_i - \hat Y_i)^2}{n-2} = \frac{\sum e_i^2}{n-2}
\]</span>
Along with that, <span class="math inline">\(s\)</span> is our estimator of standard deviation.</p>
<p>Note that the corollary of <span class="math inline">\(\bar X\)</span> when estimating the variance from a single population is <span class="math inline">\(\hat Y_i\)</span>, which is our “expected value”/“estimated mean” of <span class="math inline">\(Y_i\)</span>.</p>
<p>This estimator is unbiased; <span class="math inline">\(E(s^2) = \sigma^2\)</span>.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Insofar, we’ve learned about a statistical model that acknowledges a) the tendency of <span class="math inline">\(Y\)</span> to vary with <span class="math inline">\(X\)</span> and b) the tendency for <span class="math inline">\(Y_i\)</span> to vary at <span class="math inline">\(X_i\)</span>.</p>
<p>Given sample data with observations <span class="math inline">\((X,Y)\)</span>, we can fit a simple linear regression model with parameters defined by the least squares method to estimate the expected value of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X\)</span>, and estimate the variance of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X\)</span>.</p>
<p>However, this is just the tip of the iceberg, because we’ve yet to add one of the standard assumptions of real-world applications: normality of the error term <span class="math inline">\(\varepsilon_i\)</span>. <a href="/overview/simple-linear-regression-with-normality/">Read here.</a></p>
</div>
</div>
