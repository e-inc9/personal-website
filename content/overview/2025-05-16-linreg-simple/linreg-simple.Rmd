---
title: "Simple Linear Regression"
output: html_document
date: '2025-05-16'
categories:
  - General
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
 library(sasLM)
library(dplyr)
library(ggplot2)
library(lmtest)
library(agricolae)
library(vcdExtra)
library(car)
library(pROC)
library(leaps)
library(MVN)
library(rrcov)
library(biotools)
rm(list=ls())
set.seed(1)
```
# Mathematical and statistical relations 
Consider two variables $x,y$ that you know to be related. 

- They are related _mathematically_ if there exists a _deterministic_ relationship between the two, such that $x$ perfectly determines $y$ and vice versa. Such relationships can be expressed with a function $Y=f(X)$. One example is the relationship between inches and centimeters $Y=2.54X$. A man who is $x=65$ inches tall must be $y=165.1$ centimeters tall. 

- They are related _statistically_ if there exists a relationship between the two, but $X$ does *not* perfectly determine  $Y$ and vice versa. You know the two are related in some way -- whether visually or intuitively -- but you cannot say you know everything about $Y$ just by $X$. Take the below scatter plot relating $X$, the velocity at which a bullet is fired, and $Y$, the depth of penetration into body armor. Perhaps you didn't even have to look at the below graph to know that a relationship exists. Faster bullet $\implies$ deeper penetration.  Yet the presence of other factors at play -- angle, wind speed, freak properties of physics -- make you unable to perfectly determine penetration given velocity.

Does that mean we should just _give up_? Forget fitting a mathematical equation to this relationship? Get scared in the face of unknowing, uncertainty, the potential of being _wrong_?  Hell no. Patterns exist, trends lurk beneath; you can fit some rules, induce some structure, make some sense of it all -- potential life lesson there...

```{r, echo=FALSE}
# Create vectors
x <- c(670, 675, 679, 681, 694, 699, 699, 708, 726, 732,
       738, 740, 762, 762, 768, 780, 792, 786, 790, 787)

y <- c(66.4, 64.5, 63.6, 72.9, 79.1, 76.7, 65.5, 68.0, 57.8, 72.4,
       78.6, 87.9, 92.6, 83.0, 79.0, 75.3, 83.4, 100.7, 106.6, 112.8)

i <- 1:20

# Create dataframe
df <- data.frame(i, x, y)

ggplot(df, aes(x=x, y=y))+geom_point(size=4, shape=20,col="blue")+xlab("Velocity (m/sec)")+ylab("Penetration area (mm^2)")+theme_classic()
```

# Building up to the simple linear regression model 
Our model is best understood as a *formal counterpart* for two phenomena observed in statistical relations: 1) the inexactitude of $Y$ given $X$ and 2) the possibility of multiple $Y$'s for one $X$. Just because they exist doesn't mean we can't wrangle with them _somehow_.

## Basic setup
Our regression model starts off similar to that of a mathematical function $Y=f(X)$. However, we add an *error variable*, $\varepsilon$, to account for the existence of _other variables_ that makes the relationship between $X$ and $Y$ imperfect. 

This model assumes a relationship exists between $X$ and $Y$, but concedes that it is non-mathematical and that $Y$ is influenced by factors other than $X$. It is the formal counterpart to what we know from the graph and our intuition.

\[ Y =f(X) + \varepsilon \]

Intuition: "$Y$ is determined by $X$ and other variables represented by $\varepsilon$".

## Adding assumption of linearity 
At this stage, we must define $f(X)$, the nature of the relationship between $X$ and $Y$. It can't just be a black box function. Returning to the graph above, we could say that penetration increases linearly with velocity. The scatter does appear to mostly be a straight blob and not a curvy one.  It's simple and straightforward. $\beta_1$ is the degree to which penetration increases when velocity increases by 1 m/sec. $\beta_0$ is penetration when velocity is 0. For most purposes, $\beta_0$ isn't useful, but since we are modelling a linear relationship, there must exist a $y$-intercept.  Note that we do not dispense with $\varepsilon$; the assumption is still that there exist other variables affecting $Y$, but $X$'s effect on $Y$ is linear.
\[ Y = \beta_0 + \beta_1X_i + \varepsilon \]

Intuition: "$Y$ is determined by $X$ and other variables, but the relationship between $X$ and $Y$ is linear".

## Assumptions for the error variable 
The following assumptions are crucial in making our model useful for inference. 

- $\varepsilon$ follows a normal distribution centered at 0 with variance $\sigma^2$.
    - Thus, $E(\varepsilon)$ = 0.  We'd _expect_ that everything but $x$ does not affect $y$.
- $\varepsilon$ has the same distribution for each $x_i$.
- $\varepsilon$ is uncorrelated; one $\varepsilon_i$ is not related to another. 

There are 3 important implications that result from the above:

1. Given that $Y_i$, or $Y|X_i$ can be broken into a constant $\beta_0 + \beta_1X_i$ and $\varepsilon$, the distribution of $Y_i$ is exactly normal. 

2. $E(Y_i) = E(\beta_0+\beta_1X_i+\varepsilon) = \beta_0 + \beta_1X_i + 0$. The expected value of $Y$ at $X_i$ is solely dependent on the linear relationship we've assumed! 

3. $V(Y_i) = V(\beta_0+\beta_1X_i+\varepsilon) = V(\varepsilon) = \sigma^2$. 

Summarily,

$$ \varepsilon_i \sim N(0, \sigma^2) \implies (\beta_0 + \beta_1X_i + \varepsilon_i)\sim N(\beta_0+\beta_1X_i+0, \sigma^2)\implies Y_i =Y|X_i \sim(\beta_0 + \beta_1 X_i, \sigma^2) $$
$Y_i$ or  $Y|X_i$ is normally distributed about the line describing the linear relationship between $x$ and $y$. Its variance is the same across $X_i$'s. 

It might seem like a huge assumption to assume that $E(\epsilon) = 0$ -- in other words, that on average, we'd expect nothing else to affect $Y$ but $X$ -- but this doesn't imply that the tyexpectedpical variance about the regression line is zero. Also, note that in most real-world applications of linear regression, $X$ is not the only variable. See: [multivariate linear regression](linreg-multi.html), where once you have many variables, it's more plausible that the expected impact of "everything else" is zero. But perhaps this is a larger discussion of "expected value"...

### Linear regression model
The resulting model is:

$$ Y|X = \beta_0 + \beta_1X + \varepsilon, \varepsilon \sim  N(0, \sigma^2) \implies Y|X \sim N(\beta_0+\beta_1X, \sigma^2) $$
Which puts us in the perfect position to conduct inference on $Y_i$'s. I mean, look at the picture below. There exists a distribution for every single point!

```{r, echo=FALSE}
knitr::include_graphics('/pics/3d_distrib_linreg.png', error = FALSE)
```

# Fitting our model to sample data
If there exist sample data for which we think the above model applies ($x,y$ are statistically related in a _linear_ fashion), we then use that data to come up with the proper coefficients $\hat \beta_0, \hat \beta_1$ in $\hat Y_i = \hat \beta_0+ \hat \beta_1$.  The caret symbol denotes our estimate of $\beta_0, \beta_1$. The same logic applies to $\hat Y$.

A general rule of thumb for fitting a linear regression line (the $f(X) = \beta_0 + \beta_1x$ component of our model) is that it should run through our sample points such that the absolute distance between the line and the points in our data is minimized. This is called the *ordinary least squares (OLS)* method. We can express this distance as a sum of squared differences:

$$  \sum_{i=1}^{n} (Y_i - \hat Y_i)^2 = \sum_{i=1}^n(Y_i - (\hat \beta_0 + \hat \beta_1 X_i))^2 $$
Considering that this is a convex function, taking the first derivative of it with respect to $\hat \beta_0, \hat \beta_1$, equating both results to 0,  and solving the system of equations give us the $\hat \beta_0, \hat \beta_1$ where the sum of squared differences is minimized:

$$ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X$$ 
$$ \hat \beta_1 = \frac{\sum (X_iY_i) - n \bar X \bar Y}{ \sum X_i^2 - n \bar X^2} $$

[Full derivation here, for those interested.](/proofs_deriv/2025-05-19-linreg-coeff/linreg-coeff)
Using the two above expressions, we can find the line of "ordinary least squares" for any dataset with more than 2 observations! This is pretty significant and cool.

Moving forward, we'll use the following abbreviations.

- $\sum (X_iY_i) - n \bar X \bar Y$ as  $S_{xy}$  
- $\sum X_i^2 - n \bar X ^2$ as $S_{xx}$  
- $\sum Y_i^2 - n \bar Y^2$ as $S_{yy}$






















