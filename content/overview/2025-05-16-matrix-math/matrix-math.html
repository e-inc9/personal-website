---
title: "Intro to matrix math for multivariate stats"
date: '2025-05-16'
categories:
  - General
output: html_document
---



<div id="why-is-matrix-math-important-to-multivariate-data" class="section level2">
<h2>Why is matrix math important to multivariate data?</h2>
<p>Consider that for observations within a sample, we can record multiple variables on a single observation. For example, for 10 students sampled from a class, we can record their height, weight, and average GPA. We can represent our findings with a matrix, where a row is an observation, and each column is a variable.</p>
<pre><code>##       Height_cm Weight_kg  GPA
##  [1,]       164        75 2.29
##  [2,]       168        68 2.83
##  [3,]       186        68 2.83
##  [4,]       171        66 2.74
##  [5,]       171        61 2.30
##  [6,]       187        79 2.28
##  [7,]       175        69 2.47
##  [8,]       157        49 2.93
##  [9,]       163        71 2.53
## [10,]       166        61 3.72</code></pre>
<p>In matrix-math speak, we can denote our matrix by <span class="math inline">\(\mathbf{Y}\)</span>. We can express <span class="math inline">\(\mathbf{Y}\)</span> as 40 <span class="math inline">\(y_{ij}\)</span>’s, where <span class="math inline">\(y_{ij}\)</span> is the <span class="math inline">\(i\)</span>th observation’s <span class="math inline">\(j\)</span>th variable.
<span class="math display">\[
\mathbf{Y} =
\begin{bmatrix}
y_{11} &amp; y_{12} &amp; y_{13} \\
y_{21} &amp; y_{22} &amp; y_{23}  \\
\vdots &amp; \vdots &amp; \vdots \\
y_{i1} &amp; y_{i2} &amp; y_{i3} \\
\vdots &amp; \vdots &amp; \vdots \\
y_{10,1} &amp; y_{10,2} &amp; y_{10,3}
\end{bmatrix}
\]</span></p>
<p>We can also express <span class="math inline">\(\mathbf{Y}\)</span> as a collection of 10 <span class="math inline">\(\mathbf{y_i&#39;}\)</span> observation row vectors…
<span class="math display">\[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{y_1}&#39;\\
\mathbf{y_2}&#39;\\
\vdots \\
\mathbf{y_{10}}&#39;\\
\end{bmatrix}
\]</span></p>
<p>…where each observation row vector <span class="math inline">\(\mathbf{y_i}\)</span> is then made up of variable values on that observation only.</p>
<p><span class="math display">\[
\mathbf{y_1&#39;} =
\begin{bmatrix}
y_{11} &amp;
y_{12} &amp;
y_{13} &amp;
y_{14}
\end{bmatrix}
\]</span></p>
<p>Lastly, we can express <span class="math inline">\(\mathbf{Y}\)</span> as a collection r of 4 <span class="math inline">\(\mathbf{y}_{(j)}\)</span> variable column vectors…
<span class="math display">\[
\mathbf{Y} =
\begin{bmatrix}
\mathbf{y}_{(1)} &amp; \mathbf{y}_{(2)} &amp; \mathbf{y}_{(3)} &amp; \mathbf{y}_{(4)}
\end{bmatrix}
\]</span>
… where each variable column vector <span class="math inline">\(\mathbf{y}_{(j)}\)</span> contains the measurements for observations <span class="math inline">\(1\)</span> through <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[
\mathbf{y_{(1)}} =
\begin{bmatrix}
y_{11} \\
y_{21} \\
y_{31} \\
\vdots \\
y_{10,1}
\end{bmatrix}
\]</span></p>
<p>Note that a bolded uppercase letter represents a matrix, a bolded lowercase letter represents a column vector, and a bolded lowercase letter + tick mark represents a row vector. A non-bolded lowercase letter represents a single value, or a scalar. <span class="math inline">\(\mathbf{y}_i\)</span> denotes the ith observation vector, while <span class="math inline">\(\mathbf{y}_{(j)}\)</span> denotes the jth variable vector.</p>
<p>Representing multivariate data from a sample with matrix notation allows us to use matrix operations to express various statistics such as the sample mean and sample variance.</p>
<hr>
</div>
<div id="calculating-the-sample-mean" class="section level2">
<h2>Calculating the sample mean</h2>
<p>Recall that in the univariate case (where we only observe one variable on each observation), we calculate the sample mean by averaging: <span class="math inline">\(\sum y_i\)</span>.</p>
<p>In the multivariate case, we perform the same averaging operation for each variable to calculate the sample mean row vector <span class="math inline">\(\mathbf{\bar y&#39;}\)</span>. Let there be <span class="math inline">\(p\)</span> variables and <span class="math inline">\(n\)</span> observations.</p>
<p><span class="math display">\[
\mathbf{\bar y&#39;} =
\begin{bmatrix}
\bar{y}_1 &amp; \bar{y}_2 &amp; \cdots &amp; \bar{y}_p
\end{bmatrix}
\]</span>
We can express this mean vector as the scaled multiplication of a <span class="math inline">\(1 \times n\)</span> identity row vector <span class="math inline">\(\mathbf{j&#39;}\)</span> and sample matrix <span class="math inline">\(\mathbf{Y}\)</span>:</p>
<p><span class="math display">\[
\bar{\mathbf{y&#39;}} = \frac{1}{n} \mathbf{j&#39;} \mathbf{Y} = \frac{1}{n}
\begin{bmatrix}
1 &amp; 1 &amp; \dotsc &amp; 1
\end{bmatrix}
\cdot
\begin{bmatrix}
\mathbf{y}^{(1)} &amp; \mathbf{y}^{(2)} &amp; \dotsc &amp; \mathbf{y}^{(p)}
\end{bmatrix} =
\begin{bmatrix}
\frac{1}{n} \sum_{i=1}^{n} y_{i1} &amp;
\frac{1}{n} \sum_{i=1}^{n} y_{i2} &amp;
\cdots &amp;
\frac{1}{n} \sum_{i=1}^{n} y_{ip}
\end{bmatrix}
\]</span></p>
<p>On the other hand, if we want <span class="math inline">\(\mathbf{\bar y}\)</span> (column), we lead with the transpose of <span class="math inline">\(\mathbf{Y}&#39;\)</span>, such that the columns (variables) of <span class="math inline">\(Y\)</span> become our rows, so our product is a column with one variable per row:</p>
<p><span class="math display">\[
\mathbf{\bar y} = \frac{1}{n} \mathbf{Y&#39;} \mathbf{j} =
\frac{1}{n}
\cdot
\begin{bmatrix}
\mathbf{y}^{(1)}\\
\mathbf{y}^{(2)}\\
\dotsc \\
\mathbf{y}^{(p)}
\end{bmatrix}
\cdot
\begin{bmatrix}
1 \\ 1 \\ \dotsc \\ 1
\end{bmatrix} =
\begin{bmatrix}
\frac{1}{n} \sum_{i=1}^{n} y_{i1} \\
\frac{1}{n} \sum_{i=1}^{n} y_{i2} \\
\cdots \\
\frac{1}{n} \sum_{i=1}^{n} y_{ip}
\end{bmatrix}
\]</span>
This intuition – that to get a row as a product, lead with <span class="math inline">\(\mathbf{j&#39;}\)</span>, and to get a column as a product (with rows corresponding to columns), lead with a transpose <span class="math inline">\(\mathbf{Y&#39;}\)</span>, will explain what we see in the covariance equation.</p>
<div id="example---sample-mean" class="section level3">
<h3>Example - Sample mean</h3>
<p>Consider the following sample matrix <span class="math inline">\(\mathbf{Y}\)</span>:</p>
<pre><code>##       y1   y2   y3
##  [1,] 35  3.5 2.80
##  [2,] 35  4.9 2.70
##  [3,] 40 30.0 4.38
##  [4,] 10  2.8 3.21
##  [5,]  6  2.7 2.73
##  [6,] 20  2.8 2.81
##  [7,] 35  4.6 2.88
##  [8,] 35 10.9 2.90
##  [9,] 35  8.0 3.28
## [10,] 30  1.6 3.20</code></pre>
<p>We calculate the mean using the above math and check it with R’s sample means function.</p>
<pre class="r"><code>j_row &lt;- matrix(1, nrow=1, ncol= nrow(Y))

ybar_vec &lt;- (1/nrow(Y)) %*% j_row %*% Y; ybar_vec</code></pre>
<pre><code>##        y1   y2    y3
## [1,] 28.1 7.18 3.089</code></pre>
<pre class="r"><code>colMeans(Y)</code></pre>
<pre><code>##     y1     y2     y3 
## 28.100  7.180  3.089</code></pre>
<hr>
</div>
</div>
<div id="calculating-the-sample-covariance-matrix" class="section level2">
<h2>Calculating the sample covariance matrix</h2>
<p>Univariate variance, the scalar <span class="math inline">\(\sigma^2\)</span>, is analogous to multivariate covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>. Whereas univariate <span class="math inline">\(\sigma^2\)</span> is the sum of squared distance <span class="math inline">\(\sum (x_i - \bar{x})^2\)</span> from a sample mean for a single variable, in the multivariate case we must compute the sum of products of distance between all possible combinations of two variables, including a given variable with itself: <span class="math inline">\(\sum (x_i - \bar{x})(y_i - \bar{y})\)</span>. This results in the <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>,
<span class="math display">\[
\mathbf{\mathbf{\Sigma}} = (s_{jk}) =
\begin{pmatrix}
s_{11} &amp; s_{12} &amp; \cdots &amp; s_{1p} \\
s_{21} &amp; s_{22} &amp; \cdots &amp; s_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
s_{p1} &amp; s_{p2} &amp; \cdots &amp; s_{pp}
\end{pmatrix}
\]</span></p>
<p>Where <span class="math inline">\(s_{ab}\)</span> is the covariance between the ath and bth variable. Naturally, this results in computing the variance for all variables 1 through <span class="math inline">\(p\)</span> along the diagonal of <span class="math inline">\(\mathbf{\Sigma}\)</span>.</p>
<p>From <span class="math inline">\(s_{ab}\)</span> expressed in vector terms, we can work up to a matrix understanding of <span class="math inline">\(\mathbf{\Sigma}\)</span>. We first emphasize that the products of distance are summed over <span class="math inline">\(n\)</span> observations. Thus, we can express <span class="math inline">\(\mathbf{\Sigma}\)</span> as the scaled sum of multiple matrices <span class="math inline">\(\mathbf{M_1}, \mathbf{M_2},... \mathbf{M_n}\)</span>, where</p>
<p><span class="math display">\[
\mathbf{M_i} =  (\mathbf{y_i} - \mathbf{\bar{y}}) (\mathbf{y_i} - \mathbf{\bar{y}})&#39; =
\begin{pmatrix}
y_{i1} - \bar{y}_1 \\
y_{i2} - \bar{y}_2 \\
\vdots \\
y_{ip} - \bar{y}_p \\
\end{pmatrix}
\begin{pmatrix}
y_{i1} - \bar{y}_1 &amp; y_{i2} - \bar{y}_2 &amp; \cdots &amp; y_{ip} - \bar{y}_p
\end{pmatrix}
\]</span></p>
<p>What this means is that for observation <span class="math inline">\(j\)</span>, we subtract the <code>colmeans()</code> corresponding to each variable from <span class="math inline">\(j&#39;s\)</span> recorded values. We then multiply the resulting row <span class="math inline">\((\mathbf{y_i} - \mathbf{\bar{y}})\)</span> by the column <span class="math inline">\((\mathbf{y_i} - \mathbf{\bar{y}})&#39;\)</span> to get the product <span class="math inline">\(\sum (x_i - \bar{x})(y_i - \bar{y})\)</span> for all possible combinations of variables. This gives us a matrix <span class="math inline">\(\mathbf{M}\)</span>. We add each <span class="math inline">\(\mathbf{M}\)</span> matrix. Thus, the resulting matrix <span class="math inline">\(\sum \mathbf{M_i}\)</span> is a sum of deviations (all possible combinations of them) over <span class="math inline">\(n\)</span> observations.</p>
<p>When you scale <span class="math inline">\(\sum \mathbf{M_i}\)</span> by <span class="math inline">\(\frac{1}{n-1}\)</span>, you get the sample covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>. This allows us to calculate the covariance using the matrix alone.
<span class="math display">\[ \mathbf{\Sigma}=
\frac{1}{n-1}\sum_{i=1}^{n} (\mathbf{y_i} - \mathbf{\bar{y}}) (\mathbf{y_i} - \mathbf{\bar{y}})&#39; \]</span></p>
<p>Which can be simplified to (using the fact that <span class="math inline">\(\mathbf{(a-b)&#39; = (a&#39;-b&#39;)}\)</span>, and then multiplying):</p>
<p><span class="math display">\[
\mathbf{\Sigma}= \frac{1}{n-1}\left( \sum_{i=1}^n  \mathbf{y_i y_i&#39;} \right) - n\mathbf{\bar y \bar y&#39;}
\]</span></p>
<p>Using matrix math, the above can be expressed with the original matrix <span class="math inline">\(\mathbf{Y}\)</span>:
<span class="math display">\[\mathbf{\Sigma} = \frac{1}{n-1} \left[ \mathbf{Y&#39;Y}- \mathbf{Y&#39;} \frac{\mathbf{j}}{1} \cdot \frac{\mathbf{j&#39;}}{n} \mathbf{Y} \right]  = \frac{1}{n-1} \left[ \mathbf{Y&#39;}(\mathbf{I} - \frac{\mathbf{J}}{n})\mathbf{Y} \right] \]</span></p>
<details>
<summary>
Demonstration of the “matrix math”
</summary>
<p>Namely, that <span class="math inline">\(\sum_{i=1}^n \mathbf{y_i y_i&#39;} = \mathbf{Y&#39;Y}\)</span>…
It was driving me up a wall.</p>
<pre class="r"><code># Matrix &#39;A&#39;
a &lt;- matrix(c(1,5,2,1,3,3,4,2,5,6,9,3), byrow=TRUE,nrow=3)

# Observations $y_1... y_3$
first &lt;- as.matrix(a[1,], nrow=1); second &lt;- as.matrix(a[2,], nrow = 1); third &lt;- as.matrix(a[3,], nrow=1)</code></pre>
<p><span class="math inline">\(\sum_{i=1}^n \mathbf{y_i y_i&#39;}\)</span> uses observation rows, then adds up over observations</p>
<pre class="r"><code>first %*% t(first) + second %*% t(second) + third %*% t(third)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   35   44   59   22
## [2,]   44   70   76   29
## [3,]   59   76  101   37
## [4,]   22   29   37   14</code></pre>
<p><span class="math inline">\(\mathbf{Y&#39;Y}\)</span> uses variable columns (hence, <span class="math inline">\(\mathbf{Y&#39;}\)</span> leads)</p>
<pre class="r"><code>t(a) %*% (a)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4]
## [1,]   35   44   59   22
## [2,]   44   70   76   29
## [3,]   59   76  101   37
## [4,]   22   29   37   14</code></pre>
<p>In general, for two matrices <span class="math inline">\(A,B\)</span>, product <span class="math inline">\(AB\)</span> is equivalent to the sum of outer products formed by the <span class="math inline">\(k_{th}\)</span> column of <span class="math inline">\(A\)</span> and the <span class="math inline">\(k_{th}\)</span> row of <span class="math inline">\(B\)</span>. Thus, if the outer product has the observation <span class="math inline">\(\mathbf{y_i}\)</span> as a column multiplied by observation <span class="math inline">\(\mathbf{y_i}\)</span> as a row, then the multiplication operation must be <span class="math inline">\(\mathbf{Y&#39;Y}\)</span>.</p>
</details>
<div id="example---sample-covariance-matrix" class="section level3">
<h3>Example - Sample covariance matrix</h3>
<details>
<summary>
Take the matrix above and compute it manually. Check with R. Answers should match:
</summary>
<pre><code>##            y1        y2        y3
## y1 140.544444 49.680000 1.9412222
## y2  49.680000 72.248444 3.6760889
## y3   1.941222  3.676089 0.2501211</code></pre>
<p>Using the vector method:</p>
<pre class="r"><code>Y &lt;- data.frame(Y)
ybar_vec &lt;- colMeans(Y)
n &lt;- nrow(Y)

Sigma &lt;- matrix(0, nrow=ncol(Y), ncol=ncol(Y))
for (i in 1:n){
  dist &lt;- as.matrix(Y[i,] - ybar_vec)
  Sigma_n &lt;- t(dist) %*% dist
  Sigma &lt;- Sigma + Sigma_n
}

print(Sigma/(n-1))</code></pre>
<pre><code>##            y1        y2        y3
## y1 140.544444 49.680000 1.9412222
## y2  49.680000 72.248444 3.6760889
## y3   1.941222  3.676089 0.2501211</code></pre>
<p>Using the matrix method:</p>
<pre class="r"><code>Y&lt;- as.matrix(Y)
ybar_vec &lt;- as.matrix(ybar_vec)
Sigma &lt;-(1/(n-1)) * ( t(Y)%*%Y - n * ybar_vec %*% t(ybar_vec)); Sigma</code></pre>
<pre><code>##            y1        y2        y3
## y1 140.544444 49.680000 1.9412222
## y2  49.680000 72.248444 3.6760889
## y3   1.941222  3.676089 0.2501211</code></pre>
<p>Using more intuitive method (courtesy of ChatGPT) where the distance of each variable measurement from the respective sample mean is subtracted out:</p>
<pre class="r"><code>Y &lt;- as.matrix(Y)
Y_centered &lt;- sweep(Y,2,colMeans(Y)) #centers 
Sigma &lt;- (1/(n-1)) * ( t(Y_centered) %*% Y_centered ); Sigma</code></pre>
<pre><code>##            y1        y2        y3
## y1 140.544444 49.680000 1.9412222
## y2  49.680000 72.248444 3.6760889
## y3   1.941222  3.676089 0.2501211</code></pre>
</details>
</div>
</div>
