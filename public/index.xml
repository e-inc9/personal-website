<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on A minimal Hugo website</title>
    <link>http://localhost:4321/</link>
    <description>Recent content in Home on A minimal Hugo website</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:4321/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Lack of fit (F-test)</title>
      <link>http://localhost:4321/inference_tests/2025-10-10-lack-of-fit-f-test/lof/</link>
      <pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-10-10-lack-of-fit-f-test/lof/</guid>
      <description>&lt;p&gt;We introduce a test to determine whether a given regression (not necessarily linear) adequately fits the data.&lt;/p&gt;&#xA;&lt;div id=&#34;assumptions&#34; class=&#34;section level3&#34;&gt;&#xA;&lt;h3&gt;Assumptions&lt;/h3&gt;&#xA;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;&#xA;&lt;li&gt;Observations &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt; are independent, normally distributed, and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; has the same variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;&lt;/li&gt;&#xA;&lt;li&gt;Data have repeat observations (replicates) at one or more &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; levels (replications).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;This test involves two models. The first is a “full” model, which we compare against, and a “reduced” model, which is a given regression model. The goal is to compare the _______ of the reduced model vs. the full model through a ratio, which follows an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; distribution.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Constancy of variance</title>
      <link>http://localhost:4321/inference_tests/2025-10-06-constancy-of-variance/const_var/</link>
      <pubDate>Mon, 06 Oct 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-10-06-constancy-of-variance/const_var/</guid>
      <description>&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;Given a sample from a population, we’d often like to know whether that sample comes from a normally distributed population. We present two tests. The Brown-Forsythe test, and the Breusch-Pagan test. The former makes no assumptions about the population, the latter does make assumptions about the population.&lt;/p&gt;&#xA;&lt;div id=&#34;brown-forsythe&#34; class=&#34;section level2&#34;&gt;&#xA;&lt;h2&gt;Brown-Forsythe&lt;/h2&gt;&#xA;&lt;p&gt;Say we’d like to see if &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; has a constant variance in the population. Based on sampled &lt;span class=&#34;math inline&#34;&gt;\(y_1... y_n\)&lt;/span&gt;,&lt;/p&gt;&#xA;&lt;p&gt;We&#xA;1) divide the data into two groups consisting of &lt;span class=&#34;math inline&#34;&gt;\(y_{1i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{2i}\)&lt;/span&gt;&#xA;2) within each group, calculate &lt;span class=&#34;math inline&#34;&gt;\(d_{1i}=|y_{1i} - median(y_1)|\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d_{2i} = |y_{2i} - median(y_2)|\)&lt;/span&gt;&#xA;3) calculate the sample means of said absolute departures from each group&#xA;4) The test statistic:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Confidence intervals</title>
      <link>http://localhost:4321/inference_tests/2025-09-29-confidence-intervals/confidence-intervals/</link>
      <pubDate>Mon, 29 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-09-29-confidence-intervals/confidence-intervals/</guid>
      <description>&lt;p&gt;We begin with some obvious truths: for a given estimate &lt;code&gt;\(\hat \theta\)&lt;/code&gt; of parameter &lt;code&gt;\(\theta\)&lt;/code&gt; &amp;ndash; say &lt;code&gt;\(\bar x\)&lt;/code&gt; of &lt;code&gt;\(\mu\)&lt;/code&gt; &amp;ndash; &lt;code&gt;\(\hat \theta\)&lt;/code&gt; is virtually never &lt;code&gt;\(\theta\)&lt;/code&gt;. Furthermore, the distance of &lt;code&gt;\(\hat \theta\)&lt;/code&gt; from &lt;code&gt;\(\theta\)&lt;/code&gt; is virtually unknown. If we&amp;rsquo;re interested in &lt;code&gt;\(\theta\)&lt;/code&gt;, &lt;code&gt;\(\hat \theta\)&lt;/code&gt; is of little use on its own.&lt;/p&gt;&#xA;&lt;p&gt;We can, however, use properties of the distribution of &lt;code&gt;\(\hat \theta\)&lt;/code&gt; (which are downstream of our sample) to come up with a range, or  &lt;em&gt;interval&lt;/em&gt;, of plausible &lt;code&gt;\(\theta\)&lt;/code&gt;&amp;rsquo;s. This, surely, is more sensible than blindly trusting &lt;code&gt;\(\hat \theta\)&lt;/code&gt; as &lt;code&gt;\(\theta\)&lt;/code&gt; (or anywhere close to it).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Correlation</title>
      <link>http://localhost:4321/inference_tests/2025-09-28-t-test-correlation/t-test-corr/</link>
      <pubDate>Sun, 28 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-09-28-t-test-correlation/t-test-corr/</guid>
      <description>&lt;h2 id=&#34;correlation&#34;&gt;Correlation&lt;/h2&gt;&#xA;&lt;p&gt;We consider the definition of population correlation between variables &lt;code&gt;\(X,Y\)&lt;/code&gt;&lt;/p&gt;&#xA;$$&#xA;\rho_{XY} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}&#xA;$$&lt;h2 id=&#34;estimator&#34;&gt;Estimator&lt;/h2&gt;&#xA;&lt;p&gt;The Pearson product-moment correlation coefficient &amp;ndash; &lt;code&gt;\(r_{XY}\)&lt;/code&gt; &amp;ndash; is an estimator for &lt;code&gt;\(\rho_{XY}\)&lt;/code&gt;. It is biased, but the bias is small when &lt;code&gt;\(n\)&lt;/code&gt; is large. Like &lt;code&gt;\(\rho\)&lt;/code&gt;, it has range &lt;code&gt;\([-1, 1]\)&lt;/code&gt; and the interpretation is the same.&lt;/p&gt;&#xA;$$&#xA;r_{XY} = \frac{ \sum(X_i - \bar X)(Y_i - \bar Y)}{\sqrt{\sum(X_i - \bar X)^2}\sqrt{\sum(Y_i - \bar Y)^2} }&#xA;$$&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;When &lt;code&gt;\(X,Y\)&lt;/code&gt; are bivariate normally distributed&lt;/strong&gt; (and with appropriately large n), an appropriate test statistic is&lt;/p&gt;</description>
    </item>
    <item>
      <title>Normal correlation models </title>
      <link>http://localhost:4321/overview/2025-09-26-normal-correlation-models/correlation./</link>
      <pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/overview/2025-09-26-normal-correlation-models/correlation./</guid>
      <description>&lt;h2 id=&#34;a-key-distinction&#34;&gt;A key distinction&lt;/h2&gt;&#xA;&lt;p&gt;One key assumption in the classical derivation of &lt;a href=&#34;http://localhost:4321/inference_tests/2025-06-17-t-test-linreg/t-test-linreg/&#34;&gt;inference using simple linear regression models&lt;/a&gt; is that across repeated samples, &lt;code&gt;\(X\)&lt;/code&gt; is fixed. In other words, under this assumption, when we conduct inference using simple linreg, we&amp;rsquo;re assuming that we could repeatedly take &lt;code&gt;\(Y_i\)&lt;/code&gt; at &lt;code&gt;\(X_1, ... X_n\)&lt;/code&gt; repeatedly. Maybe (?) this is a product of the experiments that were being run way back when.&lt;/p&gt;&#xA;&lt;p&gt;However, in many observational settings, this assumption isn&amp;rsquo;t feasibly possible. For example, if we had a dataset with the mean daily temperature as the predictor, we couldn&amp;rsquo;t fix the mean daily temperature of our observations across samples, since it would also be a random variable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>F-test (ANOVA/linear regressions)</title>
      <link>http://localhost:4321/inference_tests/2025-09-24-f-test-anova-linear-regressions/f-test/</link>
      <pubDate>Wed, 24 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-09-24-f-test-anova-linear-regressions/f-test/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;http://localhost:4321/distrib/2025-09-23-simple-linear-regression-anova-cut/linreg-anova/&#34;&gt;ANOVA cut of linear regression&lt;/a&gt;, we ended on &lt;code&gt;\(MSR\)&lt;/code&gt; and &lt;code&gt;\(MSE\)&lt;/code&gt;, which are two measures of variance. &lt;code&gt;\(MSR\)&lt;/code&gt; measures the variance explained by a fitted line, while &lt;code&gt;\(MSE\)&lt;/code&gt; measures the variance of observations about that (in spite of) the fitted line.&lt;/p&gt;&#xA;&lt;p&gt;Given that they are both random variables with centers:&#xA;&lt;/p&gt;&#xA;$$&#xA;E(MSE)= \sigma^2\\&#xA;E(MSR) = \sigma^2 + \beta_1 ^2 \sum(X_i - \bar X)^2&#xA;$$&lt;p&gt;&#xA;We conclude that the distribution of &lt;code&gt;\(MSR\)&lt;/code&gt; is &lt;em&gt;to the right of&lt;/em&gt; the distribution of &lt;code&gt;\(MSE\)&lt;/code&gt; if &lt;code&gt;\(\beta_1 \ne 0\)&lt;/code&gt;, and thus computed &lt;code&gt;\(MSR\)&lt;/code&gt; is likely to be greater than &lt;code&gt;\(MSE\)&lt;/code&gt;. If &lt;code&gt;\(\beta_1 = 0\)&lt;/code&gt;, then &lt;code&gt;\(MSR\)&lt;/code&gt; is likely to equal &lt;code&gt;\(MSE\)&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple linear regression (ANOVA cut)</title>
      <link>http://localhost:4321/distrib/2025-09-23-simple-linear-regression-anova-cut/linreg-anova/</link>
      <pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/distrib/2025-09-23-simple-linear-regression-anova-cut/linreg-anova/</guid>
      <description>&lt;p&gt;Note: I encourage you to first read &lt;a href=&#34;http://localhost:4321/overview/2025-05-16-linreg-simple/linreg-simple/&#34;&gt;simple linear regression&lt;/a&gt; and then &lt;a href=&#34;http://localhost:4321/overview/simple-linear-regression-with-normality/&#34;&gt;simple linear regression (with normality)&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Here we present a different &lt;em&gt;cut&lt;/em&gt; to linear regression. We don&amp;rsquo;t build on or extend what is discussed above. Rather, we build up from scratch a different way of interpreting linear regression.  Understanding this &amp;ldquo;alternative angle&amp;rdquo; will prove crucial in future statistical problems beyond linear regression.&lt;/p&gt;&#xA;&lt;h2 id=&#34;introducing-our-y_is&#34;&gt;Introducing our &lt;code&gt;\(Y_i\)&lt;/code&gt;&amp;rsquo;s.&lt;/h2&gt;&#xA;&lt;p&gt;Consider a response variable &lt;code&gt;\(Y\)&lt;/code&gt;, and observations &lt;code&gt;\(Y_i\)&lt;/code&gt; within a sample. Inevitably, we are able to calculate &lt;code&gt;\(\bar Y\)&lt;/code&gt;. It will &amp;ldquo;run through&amp;rdquo; the middle of the sample and our &lt;code&gt;\(Y_i\)&lt;/code&gt;&amp;rsquo;s will be scattered about it. We measure this variation of our observations about the sample mean using the &lt;em&gt;total sum of squares&lt;/em&gt; (SSTO).&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum likelihood estimation</title>
      <link>http://localhost:4321/overview/2025-09-15-maximum-likelihood-estimation/mle/</link>
      <pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/overview/2025-09-15-maximum-likelihood-estimation/mle/</guid>
      <description></description>
    </item>
    <item>
      <title>Simple linear regression (with normality)</title>
      <link>http://localhost:4321/overview/simple-linear-regression-with-normality/</link>
      <pubDate>Sun, 14 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/overview/simple-linear-regression-with-normality/</guid>
      <description>&lt;p&gt;We make one additional assumption to the &lt;a href=&#34;http://localhost:4321/overview/2025-05-16-linreg-simple/linreg-simple/&#34;&gt;simple linear regression assumptions&lt;/a&gt;, which is that the error variable &lt;code&gt;\(\varepsilon_i\)&lt;/code&gt; is &lt;em&gt;normally distributed&lt;/em&gt;. Its center is still 0, and its variance is still &lt;code&gt;\(\sigma^2\)&lt;/code&gt;. It remains independent and identically distributed across levels of &lt;code&gt;\(X\)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;As a result, &lt;code&gt;\(Y_i\)&lt;/code&gt; is also normally distributed with &lt;code&gt;\(E(Y_i) = \beta_0 + \beta_1X_i, V(Y_i) = \sigma^2\)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;adding-the-assumption-of-normality&#34;&gt;Adding the assumption of normality&lt;/h2&gt;&#xA;$$ \varepsilon_i \sim N(0, \sigma^2) \implies (\beta_0 + \beta_1X_i + \varepsilon_i)\sim N(\beta_0+\beta_1X_i+0, \sigma^2)\implies Y_i =Y|X_i \sim(\beta_0 + \beta_1 X_i, \sigma^2) $$&lt;p&gt;&#xA;&lt;code&gt;\(Y_i\)&lt;/code&gt; or &lt;code&gt;\(Y|X_i\)&lt;/code&gt; is normally distributed about a line describing a&#xA;linear relationship. Its variance is the same across&#xA;&lt;code&gt;\(X_i\)&lt;/code&gt;&amp;rsquo;s.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Point estimation</title>
      <link>http://localhost:4321/overview/2025-09-13-point-estimation/point-estimation/</link>
      <pubDate>Sat, 13 Sep 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/overview/2025-09-13-point-estimation/point-estimation/</guid>
      <description>&lt;h3 id=&#34;foreword&#34;&gt;Foreword&lt;/h3&gt;&#xA;&lt;p&gt;Consider that statistics is primarily about working with samples to learn about populations. We calculate the sample mean and sample standard deviation from our sample data to estimate the mean and standard deviation of our population.&lt;/p&gt;&#xA;&lt;p&gt;This process is formally known as point estimation. Given a parameter &lt;code&gt;\(\theta\)&lt;/code&gt; (population mean, for example), we calculate a point estimate &lt;code&gt;\(\hat \theta\)&lt;/code&gt; (an actual numerical value) using a point estimator (&lt;code&gt;\(\bar x\)&lt;/code&gt;, which describes the mathematics of the sample mean).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-invasive functional brain imaging </title>
      <link>http://localhost:4321/misc/2025-06-29-fnirs-functional-near-infrared-spectroscopy/fnirs/</link>
      <pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/misc/2025-06-29-fnirs-functional-near-infrared-spectroscopy/fnirs/</guid>
      <description>&lt;h2 id=&#34;non-invasive-functional-brain-imaging&#34;&gt;Non-invasive functional Brain Imaging&lt;/h2&gt;&#xA;&lt;p&gt;Primarily falls into two categories:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hemodynamic/Metabolic&#xA;&lt;ul&gt;&#xA;&lt;li&gt;fNIRS, fMRI, POT&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Neuronal&#xA;&lt;ul&gt;&#xA;&lt;li&gt;EEGs, EPs/ERPs, MEG&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;hemodynamicmetabolic-fnirs-as-example&#34;&gt;Hemodynamic/Metabolic (fNIRS as example)&lt;/h2&gt;&#xA;&lt;p&gt;Consider:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Oxygenated blood is rich in oxyhemoglobin (&lt;code&gt;\(HbO_2\)&lt;/code&gt;) and low in deoxyhemoglobin (&lt;code&gt;\(HbO\)&lt;/code&gt;);&lt;/li&gt;&#xA;&lt;li&gt;Oxyhemoglobin and deoxyhemoglobin have been shown to absorb infrared light at certain frequencies;&lt;/li&gt;&#xA;&lt;li&gt;When performing a task, there is cerebral (oxygenated) blood flow (CBF) to regions of the brain that are active during said task;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;And that we would like to determine, for a given task, which regions of the brain are most active based on the presence of oxygenated blood.&lt;/p&gt;</description>
    </item>
    <item>
      <title>3b1b Linear Algebra</title>
      <link>http://localhost:4321/misc/2025-06-28-3b1b-/3b1b/</link>
      <pubDate>Sat, 28 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/misc/2025-06-28-3b1b-/3b1b/</guid>
      <description>&lt;h2 id=&#34;video-1-vectors&#34;&gt;Video 1 Vectors&lt;/h2&gt;&#xA;&lt;p&gt;Start in the &lt;code&gt;\((x,y)\)&lt;/code&gt; standard coordinate plane. Define the&#xA;&lt;code&gt;\(i=[1 \quad 0 ]\)&lt;/code&gt; and &lt;code&gt;\(j=[0\quad1]\)&lt;/code&gt;. Note that any &lt;strong&gt;linear combination&lt;/strong&gt;&#xA;of the two vectors &lt;code&gt;\(ai + bj\)&lt;/code&gt; could reach any point on the plane. We call&#xA;this set of achievable points the &lt;strong&gt;span&lt;/strong&gt; of the two vectors.&lt;/p&gt;&#xA;&lt;p&gt;What&amp;rsquo;s the usefulness of vectors? They can be used to represent data,&#xA;for one thing.&lt;/p&gt;&#xA;&lt;h2 id=&#34;video-2-linear-combinations-span-and-basis-vectors&#34;&gt;Video 2 Linear combinations, span, and basis vectors&lt;/h2&gt;&#xA;&lt;p&gt;Consider vector &lt;code&gt;\(a = [2 \quad 4]\)&lt;/code&gt;, which is represented in the&#xA;coordinate plane as a point 2 points to the right and four points up. We&#xA;can break &lt;code&gt;\(a\)&lt;/code&gt; down into a sum of scaled vectors &lt;code&gt;\(i, j\)&lt;/code&gt;:&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>T-test (linear regressions)</title>
      <link>http://localhost:4321/inference_tests/2025-06-17-t-test-linreg/t-test-linreg/</link>
      <pubDate>Tue, 17 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-06-17-t-test-linreg/t-test-linreg/</guid>
      <description>&lt;p&gt;Just as the estimators sample mean and sample variance follow a distribution, so do the estimators  &lt;code&gt;\(\hat \beta_0, \hat \beta_1\)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Even though it can be argued that there does not exist a &amp;ldquo;true&amp;rdquo; &lt;code&gt;\(\beta_0\)&lt;/code&gt; and &lt;code&gt;\(\beta_1\)&lt;/code&gt; in the same way a true &lt;code&gt;\(\mu, \sigma\)&lt;/code&gt; might (the former comes from an assumption-laden construct that we use to interpret reality while the other objectively exists in reality), we could say that having identified phenomena for which a linear regression is applicable, there  &lt;em&gt;does&lt;/em&gt; exist a regression line that exists for all existing &amp;ldquo;population&amp;rdquo; data on this phenomena. We&amp;rsquo;d like to infer this &lt;code&gt;\(\beta_0, \beta_1\)&lt;/code&gt; that&amp;rsquo;s &amp;ldquo;out there&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>T-test (population mean)</title>
      <link>http://localhost:4321/inference_tests/2025-06-13-t-test/t-test/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/2025-06-13-t-test/t-test/</guid>
      <description>&lt;script src=&#34;http://localhost:4321/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;&#xA;&lt;link href=&#34;http://localhost:4321/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;&#xA;&lt;script src=&#34;http://localhost:4321/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;&#xA;&lt;link href=&#34;http://localhost:4321/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;&#xA;&lt;p&gt;The t-test can be used to make inferences regarding a population mean when we have a sample from said population.&lt;/p&gt;&#xA;&lt;p&gt;The assumptions are that&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The sample mean follows an approximately normal distribution. This requires either &lt;code&gt;\(n \ge 30\)&lt;/code&gt; or the population to be normally distributed, either of which are ensures &lt;code&gt;\(\bar X \sim N\)&lt;/code&gt; (approximately).&lt;/li&gt;&#xA;&lt;li&gt;The numerator and denominator are independent, which can be assumed for a sample drawn from a normal population.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;The statistic used is&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deriving the t pdf</title>
      <link>http://localhost:4321/proofs_deriv/2025/05/27/deriving-the-t-pdf/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/proofs_deriv/2025/05/27/deriving-the-t-pdf/</guid>
      <description>&lt;p&gt;By definition, a t-distributed random variable with &lt;code&gt;\(\nu\)&lt;/code&gt; degrees of freedom is the ratio of a standard normal variable &lt;code&gt;\(Z \sim N(0,1)\)&lt;/code&gt; over the square root of a chi-squared variable &lt;code&gt;\(Y \sim \chi^2_{\nu}\)&lt;/code&gt; divided by its degrees of freedom.&lt;/p&gt;&#xA;\[&#xA;T_\nu = \frac{Z}{\sqrt{{Y \over \nu}}}, \quad Z \perp Y&#xA;\]&lt;p&gt;This is a lengthy derivation, so buckle up. We start with the cdf. We isolate &lt;code&gt;\(Z\)&lt;/code&gt; since its upper and lower bounds make the inner integral easier to work with (i.e. for Leibniz&amp;rsquo; rule, the derivative of the upper bound is easy to compute).&lt;/p&gt;</description>
    </item>
    <item>
      <title>T-distribution</title>
      <link>http://localhost:4321/distrib/2025-05-27-t-distribution/t-dist/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/distrib/2025-05-27-t-distribution/t-dist/</guid>
      <description>&lt;p&gt;A t-distributed random variable with &lt;code&gt;\(\nu\)&lt;/code&gt; degrees of freedom is the ratio of a standard normal variable &lt;code&gt;\(Z \sim N(0,1)\)&lt;/code&gt; over the square root of a chi-squared variable &lt;code&gt;\(Y \sim \chi^2_{\nu}\)&lt;/code&gt; divided by its degrees of freedom.&lt;/p&gt;&#xA;$$&#xA;T_\nu = \frac{Z}{\sqrt{{Y \over \nu}}}, \quad Z \perp Y&#xA;$$&lt;p&gt;Its pdf is given by&lt;/p&gt;&#xA;\[ f(t) = \frac{1}{\sqrt{\pi\nu} \Gamma({\nu \over 2})}\&#xA;\cdot \frac{1}{\left({t^2 \over \nu} + 1\right)^{\frac{\nu+1}{2}} } \cdot \Gamma\left({\nu+1 \over 2}\right) \]&lt;p&gt;Its expected variance and mean are:&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distribution of hat beta_0</title>
      <link>http://localhost:4321/proofs_deriv/2025/05/23/distribution-of-hat-beta_0/</link>
      <pubDate>Fri, 23 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/proofs_deriv/2025/05/23/distribution-of-hat-beta_0/</guid>
      <description>&lt;h2 id=&#34;deriving-distribution-of-hat-beta_0&#34;&gt;Deriving distribution of &lt;code&gt;\(\hat \beta_0\)&lt;/code&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Assume that &lt;code&gt;\(Y_i \sim N(\beta_0 + \beta_1X_i, \sigma^2)\)&lt;/code&gt; as a result of the definition &lt;code&gt;\(Y_i= \beta_0 + \beta_1 X_i + \varepsilon_i\)&lt;/code&gt;, where each &lt;code&gt;\(\varepsilon_i \sim N(0, \sigma^2)\)&lt;/code&gt;. This is what&amp;rsquo;s assumed under the simple linear regression model,  &lt;strong&gt;including the additional assumption that &lt;code&gt;\(\varepsilon\)&lt;/code&gt; is normally distributed (necessary for inference).&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Recall that given a sample, &lt;code&gt;\(\hat \beta_0 = \bar Y - \hat \beta_1 \bar X\)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Since &lt;code&gt;\(\hat \beta_1 \sim N(\beta_1, \frac{\sigma^2}{S_{xx}})\)&lt;/code&gt;, and &lt;code&gt;\(\bar X\)&lt;/code&gt; is a constant determined by the sample and &lt;code&gt;\(\bar Y\)&lt;/code&gt; is normally distributed, &lt;code&gt;\(\hat \beta_0\)&lt;/code&gt; is normally distributed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deriving the chi-squared pdf</title>
      <link>http://localhost:4321/proofs_deriv/2025/05/20/deriving-the-chi-squared-pdf/</link>
      <pubDate>Tue, 20 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/proofs_deriv/2025/05/20/deriving-the-chi-squared-pdf/</guid>
      <description>&lt;h1 id=&#34;the-chi-squared-distribution&#34;&gt;The chi-squared distribution&lt;/h1&gt;&#xA;&lt;p&gt;By definition, the chi-squared distribution is the sum of &lt;code&gt;\(k\)&lt;/code&gt; standard normal variables squared. Let &lt;code&gt;\(X\)&lt;/code&gt; be a chi-squared variable with &lt;code&gt;\(k\)&lt;/code&gt; degrees of freedom. Then,&lt;/p&gt;&#xA;$$&#xA;X= \sum_{i=1}^k Z_i^2&#xA;$$&lt;p&gt;Our objective is to define its distribution. We first start with the pdf when &lt;code&gt;\(k=1\)&lt;/code&gt;. Then, seeing that its pdf is that of a Gamma, we find the pdf of a sum of Gammas. Moment generating functions come into play.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deriving the coefficients for linear regression</title>
      <link>http://localhost:4321/proofs_deriv/2025/05/19/deriving-the-coefficients-for-linear-regression/</link>
      <pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/proofs_deriv/2025/05/19/deriving-the-coefficients-for-linear-regression/</guid>
      <description>&lt;h2 id=&#34;finding-coefficients-for-linear-regression&#34;&gt;Finding coefficients for linear regression&lt;/h2&gt;&#xA;&lt;p&gt;Recall that given a sample with observations &lt;code&gt;\(Y_1, Y_2, \dotsc Y_n\)&lt;/code&gt;, the sum of squared differences is given by:&lt;/p&gt;&#xA;$$  &#xA;\sum_{i=1}^{n} (Y_i - \hat Y_i)^2 = \sum_{i=1}^n(Y_i - (\hat \beta_0 + \hat \beta_1 X_i))^2  &#xA;$$&lt;p&gt;We denote this sum by &lt;code&gt;\(h(\hat \beta_0, \hat \beta_1)\)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;We use the first derivative to find the coefficients that minimize this sum. Since this function is convex, we know the first derivative is a minimum.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distribution of hat beta_1</title>
      <link>http://localhost:4321/proofs_deriv/2025/05/19/distribution-of-hat-beta_1/</link>
      <pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/proofs_deriv/2025/05/19/distribution-of-hat-beta_1/</guid>
      <description>&lt;p&gt;We want to know the distribution of &lt;code&gt;\(\hat \beta_1\)&lt;/code&gt; across all possible samples where we treat the &lt;code&gt;\(X_i\)&lt;/code&gt;&amp;rsquo;s as fixed each time.&lt;/p&gt;&#xA;&lt;p&gt;Assume that &lt;code&gt;\(Y_i \sim(\beta_0 + \beta_1X_i, \sigma^2)\)&lt;/code&gt; as a result of the definition&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;\(Y_i= \beta_0 + \beta_1 X_i + \varepsilon_i\)&lt;/code&gt;, where each &lt;code&gt;\(\varepsilon_i \sim N(0, \sigma^2)\)&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;p&gt;This is what&amp;rsquo;s assumed under the simple linear regression model, &lt;strong&gt;including the additional assumption that &lt;code&gt;\(\varepsilon\)&lt;/code&gt; is normally distributed (necessary for inference).&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Intro to matrix math for multivariate stats</title>
      <link>http://localhost:4321/overview/2025-05-16-matrix-math/matrix-math/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/overview/2025-05-16-matrix-math/matrix-math/</guid>
      <description>&lt;h2 id=&#34;why-is-matrix-math-important-to-multivariate-data&#34;&gt;Why is matrix math important to multivariate data?&lt;/h2&gt;&#xA;&lt;p&gt;Consider that for observations within a sample, we can record multiple variables on a single observation. For example, for 10  students sampled from a class, we can record their height, weight, and average GPA. We can represent our findings with a matrix, where a row is an observation, and each column is a variable.&lt;/p&gt;&#xA;&lt;pre&gt;&lt;code&gt;##       Height_cm Weight_kg  GPA&#xA;##  [1,]       164        75 2.29&#xA;##  [2,]       168        68 2.83&#xA;##  [3,]       186        68 2.83&#xA;##  [4,]       171        66 2.74&#xA;##  [5,]       171        61 2.30&#xA;##  [6,]       187        79 2.28&#xA;##  [7,]       175        69 2.47&#xA;##  [8,]       157        49 2.93&#xA;##  [9,]       163        71 2.53&#xA;## [10,]       166        61 3.72&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;p&gt;In matrix-math speak, we can denote our matrix by &lt;code&gt;\(\mathbf{Y}\)&lt;/code&gt;. We can express &lt;code&gt;\(\mathbf{Y}\)&lt;/code&gt; as 40 &lt;code&gt;\(y_{ij}\)&lt;/code&gt;&amp;rsquo;s, where &lt;code&gt;\(y_{ij}\)&lt;/code&gt; is the &lt;code&gt;\(i\)&lt;/code&gt;th observation&amp;rsquo;s &lt;code&gt;\(j\)&lt;/code&gt;th variable.&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple linear regression</title>
      <link>http://localhost:4321/overview/2025-05-16-linreg-simple/linreg-simple/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/overview/2025-05-16-linreg-simple/linreg-simple/</guid>
      <description>&lt;h2 id=&#34;mathematical-and-statistical-relations&#34;&gt;Mathematical and statistical relations&lt;/h2&gt;&#xA;&lt;p&gt;Consider two variables &lt;code&gt;\(x,y\)&lt;/code&gt; that you know to be related.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;They are related &lt;em&gt;mathematically&lt;/em&gt; if there exists a &lt;em&gt;deterministic&lt;/em&gt;&#xA;relationship between the two, such that &lt;code&gt;\(x\)&lt;/code&gt; perfectly determines &lt;code&gt;\(y\)&lt;/code&gt;&#xA;and vice versa. Such relationships can be expressed with a function&#xA;&lt;code&gt;\(Y=f(X)\)&lt;/code&gt;. One example is the relationship between inches and&#xA;centimeters &lt;code&gt;\(Y=2.54X\)&lt;/code&gt;. A man who is &lt;code&gt;\(x=65\)&lt;/code&gt; inches tall must be&#xA;&lt;code&gt;\(y=165.1\)&lt;/code&gt; centimeters tall.&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;They are related &lt;em&gt;statistically&lt;/em&gt; if there exists a relationship&#xA;between the two, but &lt;code&gt;\(X\)&lt;/code&gt; does &lt;em&gt;not&lt;/em&gt; perfectly determine &lt;code&gt;\(Y\)&lt;/code&gt; and vice&#xA;versa. You know the two are related in some way &amp;ndash; whether visually&#xA;or intuitively &amp;ndash; but you cannot say you know everything about &lt;code&gt;\(Y\)&lt;/code&gt;&#xA;just by &lt;code&gt;\(X\)&lt;/code&gt;. Take the below scatter plot relating &lt;code&gt;\(X\)&lt;/code&gt;, the velocity&#xA;at which a bullet is fired, and &lt;code&gt;\(Y\)&lt;/code&gt;, the depth of penetration into&#xA;body armor. Perhaps you didn&amp;rsquo;t even have to look at the below graph&#xA;to know that a relationship exists. Faster bullet &lt;code&gt;\(\implies\)&lt;/code&gt; deeper&#xA;penetration. Yet the presence of other factors at play &amp;ndash; angle,&#xA;wind speed, freak properties of physics &amp;ndash; make you unable to&#xA;perfectly determine penetration given velocity.&lt;/p&gt;</description>
    </item>
    <item>
      <title> Chapter 1 - What is Deep Learning?</title>
      <link>http://localhost:4321/misc/2025-06-01-dlp/dlp-1r/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/misc/2025-06-01-dlp/dlp-1r/</guid>
      <description>&lt;div style=&#34;background-color:#f0f0f0; padding:10px; border-left:4px solid #007ACC; margin-bottom:20px;&#34;&gt;&#xA;&lt;strong&gt;Image credit:&lt;/strong&gt; Select figures reproduced from &#xA;&lt;a href=&#34;https://www.manning.com/books/deep-learning-with-python&#34; target=&#34;_blank&#34;&gt;&#xA;&lt;em&gt;Deep Learning with Python&lt;/em&gt; by François Chollet&lt;/a&gt; (Manning Publications, 2017). &#xA;All rights reserved by the original publisher.&#xA;&lt;/div&gt;&#xA;&lt;p&gt;&lt;strong&gt;Drawing some distinctions:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Computer programming tells the computer the &amp;ldquo;rules&amp;rdquo; to obtain answers&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Rules + data =&amp;gt; answers&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Machine learning learns “statistical” rules from the answers&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Data + answers =&amp;gt; rules&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Machine learning works with large, complex data sets – is an engineering discipline. Statistics works with smaller data sets  &amp;ndash;  is a mathematical field. ML &lt;code&gt;\(\ne\)&lt;/code&gt; statistics; rather, it emerges from statistics at most&lt;/p&gt;</description>
    </item>
    <item>
      <title>ANOVA (in progress)</title>
      <link>http://localhost:4321/inference_tests/anova/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inference_tests/anova/</guid>
      <description>&lt;h1 id=&#34;intro&#34;&gt;Intro&lt;/h1&gt;&#xA;&lt;p&gt;ANOVA stands for analysis of variance. The goal of ANOVA is, for a variable measured on more than one group of data, to &lt;strong&gt;use the variance of our measurements&lt;/strong&gt; to determine whether the population mean of those groups is different between at least one pair of groups.&lt;/p&gt;&#xA;\[ H_0: \mu_1 = \mu_2 = ... =\mu_n \]&lt;p&gt;&#xA;&lt;/p&gt;&#xA;\[ H_a: \mu_i \ne \mu_j \text{ for } i \ne j \]&lt;p&gt;You might wonder why we are analyzing &lt;em&gt;variance&lt;/em&gt; to get at a truth involving means &amp;ndash; I certainly did. The best way to intuit &lt;em&gt;why&lt;/em&gt; is through a simple exercise.&lt;/p&gt;</description>
    </item>
    <item>
      <title>My thoughts on statistics</title>
      <link>http://localhost:4321/misc/2025-04-26-on-stats/on-stats/</link>
      <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/misc/2025-04-26-on-stats/on-stats/</guid>
      <description>&lt;p&gt;By definition, a statistic is information that can be gleaned from a sample. It can result from a mathematical operation performed on a sample (averaging), an arbitrary selection within it (every 5th value), or nonsensical manipulation of it (subtract 100 from each value and convert it to a string based on the starting letter of the value when converted to words).&lt;/p&gt;&#xA;&lt;p&gt;Statistics (note the s!) as a field is concerned with using a statistic from a sample to learn about the larger population from which the sample originated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About this website</title>
      <link>http://localhost:4321/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/about/</guid>
      <description></description>
    </item>
  </channel>
</rss>
