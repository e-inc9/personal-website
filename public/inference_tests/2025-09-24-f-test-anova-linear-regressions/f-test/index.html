<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>F-test (ANOVA/linear regressions) | A minimal Hugo website</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/categories/">Categories</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">F-test (ANOVA/linear regressions)</span></h1>

<h2 class="date">2025/09/24</h2>
</div>

<main>
<p>In the <a href="/distrib/2025-09-23-simple-linear-regression-anova-cut/linreg-anova/">ANOVA cut of linear regression</a>, we ended on <code>\(MSR\)</code> and <code>\(MSE\)</code>, which are two measures of variance. <code>\(MSR\)</code> measures the variance explained by a fitted line, while <code>\(MSE\)</code> measures the variance of observations about that (in spite of) the fitted line.</p>
<p>Given that they are both random variables with centers:
</p>
$$
E(MSE)= \sigma^2\\
E(MSR) = \sigma^2 + \beta_1 ^2 \sum(X_i - \bar X)^2
$$<p>
We conclude that the distribution of <code>\(MSR\)</code> is <em>to the right of</em> the distribution of <code>\(MSE\)</code> if <code>\(\beta_1 \ne 0\)</code>, and thus computed <code>\(MSR\)</code> is likely to be greater than <code>\(MSE\)</code>. If <code>\(\beta_1 = 0\)</code>, then <code>\(MSR\)</code> is likely to equal <code>\(MSE\)</code>.</p>
<p><strong>Note that this intuition cannot be understood (visually) from an image, as <code>\(MSE\)</code> is <code>\(\sum( Y_i - \hat Y_i)^2\)</code> DIVIDED by <code>\(n-2\)</code>!</strong></p>
<h3 id="the-f-statistic">The F-statistic</h3>
<p>We test the likelihood of <code>\(\beta_1 = 0\)</code> using <code>\(F = \frac{MSR}{MSE}\)</code>.</p>
<p>As discussed above, if <code>\(\beta_1 = 0\)</code>, then <code>\(MSR\)</code> is likely to equal <code>\(MSE\)</code>, which means <code>\(F\)</code> is likely to be 1. If <code>\(\beta_1 \ne 0\)</code>, then <code>\(MSR\)</code> is likely to be greater than <code>\(MSE\)</code>. The greater the departure of <code>\(\beta_1\)</code> from <code>\(0\)</code>, the larger <code>\(F\)</code> is likely to be.</p>
<p>When we use the words &ldquo;likely&rdquo; and &ldquo;likelihood&rdquo;, we imply a probabilistic distribution. However, what distribution, exactly, does this ratio follow?</p>
<p>The ratio of <code>\(MSR\)</code> over <code>\(MSE\)</code> is an [F-distributed](link here) variable with (<code>\(1, n-2\)</code>) degrees of freedom.</p>
<p><code>$$\frac{\frac{SSR}{\sigma^2}}{1} \div \frac{\frac{SSE}{\sigma^2}}{n-2} = \frac{MSR}{MSE} \sim F(1, n-2)$$</code></p>
<details>
<summary> Derivation here. </summary>
By Cochran's theorem, if `\(n\)` observations `\(Y_i\)` come from the same normal distribution with mean `\(\mu\)` and variance `\(\sigma^2\)`, and `\(SSTO = \sum (Y_i - \bar Y)^2\)` is partitioned into `\(k\)` sums of squares `\(SS_1... SS_k\)`, each with degrees of freedom `\(df_{1}...df_k\)`, then `\(\frac{SS_1}{\sigma^2} ... \frac{SS_k}{\sigma^2}\)` are independent `\(\chi^2\)` variables with `\(df_r\)` degrees of freedom.
<p>Under the null hypothesis where <code>\(\beta_1 = 0\)</code>, each observation <code>\(Y_i\)</code> has <code>\(E(Y_i) = \beta_0 = \mu\)</code> (contrast this with <code>\(\beta_1 \ne 0\)</code>, where we expect the mean response to vary with the predictor). Owing to the assumptions of linear regression, variance is constant <code>\(\sigma^2\)</code>. <code>\(SSTO\)</code>, as we know, is partitioned into <code>\(SSR\)</code> and <code>\(SSE\)</code>, with 1 and <code>\(n-2\)</code> degrees of freedom, respectively.</p>
<p>Then, <code>\(\frac{SSR}{\sigma^2}, \frac{SSE}{\sigma^2}\)</code> are independent and follow <code>\(\chi^2\)</code> distributions with 1 and <code>\(n-2\)</code> degrees of freedom, respectively. If we then divide these two fractions by their degrees of freedom, we obtain:</p>
<p><code>$$\frac{\frac{SSR}{\sigma^2}}{1}, \frac{\frac{SSE}{\sigma^2}}{n-2}$$</code></p>
<p>When we take the ratio of these two fractions, by the definition of an F-distributed random variable, we obtain an F-distributed random variable with <code>\((1, n-2)\)</code> degrees of freedom.</p>
</details> 
<details>
<summary> We examine whether `\(\hat \beta_1=0\)` for the fitted line for the Toluca company. </summary>
<pre><code class="language-r">library(api2lm)

lm1 &lt;- lm(work_hours~lot_size, data=toluca)
anova(lm1)
</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: work_hours
##           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## lot_size   1 252378  252378  105.88 4.449e-10 ***
## Residuals 23  54825    2384                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
<p>Judging by the F-value of 105.88 in the ANOVA table (corresponding to <code>\(p &lt; .05\)</code>), we reject the null hypothesis of <code>\(\beta_1 = 0\)</code>. The (large) amount of variance explained by <code>\(X\)</code> is unlikely under conditions where <code>\(\beta_1 = 0\)</code>.</p>
</details>
<h3 id="relation-to-the-t-test">Relation to the t-test</h3>
<p>If we revisit <code>\(F = \frac{MSR}{MSE}\)</code>, we can express it in terms of a <code>\(t\)</code> statistic. We make use of the following identities:
</p>
$$
SSR =\sum(\hat Y_i - \bar Y)^2 = \sum(\hat \beta_0 + \hat \beta_1 X_i - (\hat \beta_0 + \hat \beta_1 \bar X))^2 = \sum(\hat \beta_1 X_i - \hat \beta_1 \bar X)^2 = \hat \beta_1 ^2 \sum (X_i - \bar X)^2 = MSR \quad \text{(divide by 1 df)} \\
s^2({\hat \beta_1}) = \frac{MSE}{\sum(X_i - \bar X)^2} \quad \text{our estimate of } \sigma^2(\hat \beta_1)
$$$$ \begin{aligned}
F &= \frac{MSR}{MSE} \sim F(1, n-2) \\
&= \frac{\hat \beta_1^2 \sum(X_i - \bar X)^2}{s^2(\hat \beta_1) \sum (X_i - \bar X)^2} = \left(\frac{\hat \beta_1}{s({\hat \beta_1})} \sim t(1)\right)^2  \\
\end{aligned}$$<p>`</p>
<p>In other words, an <code>\(F\)</code> statistic (used for testing <code>\(\beta_1 = 0\)</code>) is equivalent to a squared <code>\(t\)</code> statistic. That means that given a <code>\(t\)</code> statistic, you can square it to get an F-statistic.  Given an <code>\(F\)</code> statistic, you take the square root to get a t-statistic.</p>
<p>However, what is the <em>probabilistic</em> relation of <code>\(t\)</code> to <code>\(F\)</code>? We define it below. We&rsquo;ve established that since <code>\(F = T^2\)</code>, <code>\(P(F &gt; t_0) = P(T^2 &gt; t_0)\)</code>.
Then,</p>
<p><code>$$\begin{aligned} P(F &gt; t_0) &amp;= P(T^2 &gt; t_0) \\ P(F &gt; t_0) &amp;= P(T &lt; -\sqrt t_0) + P(T &gt; \sqrt t_0) \\ P(F &gt; t_0) &amp;= 2P(T &gt; \sqrt t_0) \quad \text{ (owing to symmetry of T) } \end{aligned}$$</code></p>
<p>We conclude that the probabilistic relation is that the probability of <code>\(F\)</code> being greater than <code>\(t_0\)</code> is equivalent to the probability of <code>\(t\)</code> being less than <code>\(-\sqrt{t_0}\)</code> or greater than <code>\(\sqrt{t_0}\)</code>.</p>
<p><strong>In other words, a one-sided F-test using <code>\(F\)</code> is equivalent to a two-sided t-test using <code>\(T\)</code>.</strong></p>
<pre><code class="language-r">pf(105.88, df1=1, df2=nrow(toluca)-2, lower.tail=FALSE)
</code></pre>
<pre><code>## [1] 4.44711e-10
</code></pre>
<pre><code class="language-r">pt(sqrt(105.88), df=nrow(toluca)-2, lower.tail=FALSE)*2
</code></pre>
<pre><code>## [1] 4.44711e-10
</code></pre>

</main>

  <footer>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/katex/dist/katex.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/katex/dist/contrib/auto-render.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/render-katex.js" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>

  
  <hr/>
  Â© 2025
  
  </footer>
  </body>
</html>

